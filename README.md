# Detailed Documentation: Implementing Medusa in the Exo Framework
# Medusa Ultra Enhanced: Medusa Decoder with Probability Threshold Filtering

## Enhanced Features

- **Probability Threshold Filtering**: Filter out low-confidence token predictions from Medusa heads based on a configurable probability threshold
- **Debug Mode**: Detailed logging of filtered tokens and probabilities for debugging
- **Demonstration Mode**: Easy testing of filtering functionality

## Usage

```bash
# Set the probability threshold (0.0 to 1.0)
MEDUSA_PROBABILITY_THRESHOLD=0.8 

# Enable debug output
MEDUSA_DEBUG=true 

# Force filtering demonstration
MEDUSA_FORCE_TEST=true 

# Run with the enhanced Medusa decoder
exo run medusa-v1.0-vicuna-7b-v1.5 --inference-engine torch --prompt "Your prompt here" --medusa-enable
```

## Benefits

- Improved output quality by filtering out low-confidence predictions
- More control over the token generation process
- Detailed insights into the Medusa decoding process

---

## Overview
We implemented Medusa, a technique for faster inference through parallel token prediction. This documentation details all changes made to enable Medusa support.

## 1. Modified `TorchDynamicShardInferenceEngine` Initialization

**File:** `exo/inference/torch/sharded_inference_engine.py`

**Change:** Added Medusa-specific attributes to the init method

```python
def __init__(self, shard_downloader: ShardDownloader):
    # Original code...
    
    # Added new fields for Medusa support
    self.model = None  # Direct model reference for Medusa
    self.is_medusa_model_loaded = False  # Flag to track if current model uses Medusa
```

**Purpose:** These attributes track if a Medusa model is loaded and maintain a direct reference to the model for the Medusa decoder.

## 2. Enhanced `ensure_shard` Method

**File:** `exo/inference/torch/sharded_inference_engine.py`

**Change:** Modified method to detect and handle Medusa models

```python
async def ensure_shard(self, shard: Shard):
    # Existing code...
    
    # Added detection for Medusa models
    from exo.models import model_cards
    self.is_medusa_model_loaded = model_cards.get(shard.model_id, {}).get("is_medusa", False)
    
    # Added branching logic for loading method
    if self.is_medusa_model_loaded:
        if DEBUG >= 1:
            print(f"Loading Medusa-specific model: {shard.model_id}")
        # Load Medusa model with specialized code path
        await self._load_medusa_model(shard)
    else:
        # Load regular model
        await self._load_standard_model(shard)
```

**Purpose:** This change checks if the requested model has Medusa capability by examining the `is_medusa` flag in the model cards registry. If it's a Medusa model, it uses a specialized loading path.

**Example:** When loading "medusa-v1.0-vicuna-7b-v1.5", it detects the model has `"is_medusa": True` in the models registry and uses the Medusa-specific loader.

## 3. Implemented Specialized Medusa Model Loader

**File:** `exo/inference/torch/sharded_inference_engine.py`

**Change:** Added a new method for loading Medusa models

```python
async def _load_medusa_model(self, shard: Shard):
    """Specialized loader for Medusa models that includes medusa-specific components"""
    from transformers import AutoModelForCausalLM
    
    def load_medusa():
        try:
            # Import required libraries
            import torch
            
            # Direct transformers loading to preserve Medusa heads
            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=self.model_config["torch_dtype"],
                device_map="auto",
                trust_remote_code=True  # Required for custom model code
            )
            
            # Set model to evaluation mode
            model.eval()
            
            # Import and configure Medusa decoder with parameters from CLI
            from exo.inference.medusa_decoder import MedusaDecoder
            medusa_config = self.get_medusa_config()
            
            medusa_decoder = MedusaDecoder(
              model=model,
              tokenizer=self.tokenizer,
              medusa_heads=medusa_config["heads"],
              tree_size=medusa_config["tree_size"],
              max_candidates=medusa_config["candidates"]
            )
            
            # Create compatibility wrapper
            class MedusaModelWrapper:
                def __init__(self, model, decoder, device, max_tokens=2048):
                    self.model = model
                    self.medusa_decoder = decoder
                    self.device = device
                    self.max_generated_tokens = max_tokens
                
                # Compatibility method to match API
                def generate(self, tokens, **kwargs):
                    batch_size = tokens.shape[0]
                    vocab_size = getattr(self.model.config, "vocab_size", 32000)
                    logits = torch.zeros((batch_size, 1, vocab_size), device=self.device)
                    return None, logits
            
            # Store decoder and return wrapper
            self.medusa_decoder = medusa_decoder
            return MedusaModelWrapper(model, medusa_decoder, self.device)
        
        except Exception as e:
            print(f"Error loading Medusa model: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    # Execute loading in executor
    self.sharded_model = await asyncio.get_running_loop().run_in_executor(
      self.executor,
      load_medusa
    )
    
    # Store direct model reference
    if hasattr(self.sharded_model, 'model'):
        self.model = self.sharded_model.model
```

**Purpose:** This method implements specialized loading for Medusa models:
1. Uses `AutoModelForCausalLM` to preserve Medusa's extra prediction heads
2. Creates a `MedusaDecoder` with configuration parameters from CLI
3. Wraps the model in a compatibility layer for the existing API
4. Stores direct references for later use

**Example:** When a user runs `exo run medusa-v1.0-vicuna-7b-v1.5 --medusa-enable --medusa-heads 4 --medusa-tree-size 5 --medusa-candidates 5`, those parameters are used to configure the MedusaDecoder.

## 4. Override `infer_prompt` Method

**File:** `exo/inference/torch/sharded_inference_engine.py`

**Change:** Added an override that routes to Medusa-specific inference

```python
async def infer_prompt(self, request_id: str, shard: Shard, prompt: str, inference_state: Optional[dict] = None) -> tuple[np.ndarray, Optional[dict]]:
    """Override the parent class to provide direct Medusa generation for Medusa models"""
    # If this is a Medusa model and Medusa is enabled, use specialized path
    if self.is_medusa_model_loaded and self.is_medusa_enabled():
        if DEBUG >= 1:
            print(f"Using Medusa direct generation for model: {shard.model_id}")
        return await self._medusa_infer_prompt(request_id, shard, prompt, inference_state)
    
    # Otherwise use standard implementation from parent class
    return await super().infer_prompt(request_id, shard, prompt, inference_state)
```

**Purpose:** This method checks if both:
1. A Medusa model is loaded
2. The Medusa feature is enabled via CLI flag (--medusa-enable)

If both conditions are true, it uses a specialized inference path. Otherwise, it falls back to standard inference.

**Example:** When running with `--medusa-enable`, the code path branches to `_medusa_infer_prompt`.

## 5. Implemented Medusa-specific Inference Method

**File:** `exo/inference/torch/sharded_inference_engine.py`

**Change:** Added Medusa-specific inference implementation

```python
async def _medusa_infer_prompt(self, request_id: str, shard: Shard, prompt: str, inference_state: Optional[dict] = None) -> tuple[np.ndarray, Optional[dict]]:
    """Specialized implementation for Medusa models using the decoder directly"""
    # Ensure model is loaded
    await self.ensure_shard(shard)
    
    if not hasattr(self, 'medusa_decoder') or self.medusa_decoder is None:
        if DEBUG >= 1:
            print("Medusa decoder not initialized, falling back to standard inference")
        return await super().infer_prompt(request_id, shard, prompt, inference_state)
    
    # Define generation function for separate thread
    def generate_with_medusa():
        try:
            # Encode input tokens
            device = self.device
            input_tokens = self.tokenizer.encode(prompt, return_tensors="pt").to(device)
            
            # Set generation parameters
            max_new_tokens = min(self._medusa_tree_size * 20, 256)
            temperature = 0.0  # Default to greedy for Medusa
            
            # Use Medusa decoder for parallel generation
            with torch.no_grad():
                generated_ids = self.medusa_decoder.generate(
                    input_tokens,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature
                )
            
            # Convert to numpy for compatibility with rest of system
            if isinstance(generated_ids, torch.Tensor):
                output_array = generated_ids.cpu().numpy()
            else:
                output_array = np.array(generated_ids)
                
            return output_array, input_tokens.shape[1]
            
        except Exception as e:
            print(f"Error in Medusa generation: {e}")
            import traceback
            traceback.print_exc()
            return None, 0
    
    # Run generation in separate thread
    output_array, input_length = await asyncio.get_running_loop().run_in_executor(
        self.executor,
        generate_with_medusa
    )
    
    if output_array is None:
        # Handle failed generation
        vocab_size = getattr(self.tokenizer, 'vocab_size', 32000)
        dummy_logits = np.zeros((1, vocab_size), dtype=np.float32)
        return dummy_logits, inference_state
    
    # Extract generated tokens (excluding input)
    if output_array.shape[1] > input_length:
        # Normal case: output includes input + new tokens
        generated_tokens = output_array[:, input_length:]
    else:
        # Edge case: output might just be the new tokens
        generated_tokens = output_array
    
    # Create dummy logits and store the actual tokens in inference_state
    vocab_size = getattr(self.tokenizer, 'vocab_size', 32000)
    first_token_logits = np.zeros((1, vocab_size), dtype=np.float32)
    
    if inference_state is None:
        inference_state = {}
    
    inference_state["medusa_output"] = generated_tokens
    inference_state["is_medusa_output"] = True
    
    return first_token_logits, inference_state
```

**Purpose:** This is the core implementation of Medusa generation:
1. It tokenizes the input text
2. Calls the Medusa decoder's `generate` method, which performs the actual parallel prediction
3. Extracts the generated tokens and stores them in the inference state
4. Returns a dummy logits array to maintain API compatibility, while the actual tokens are passed via inference_state

**Example:** When generating text, this calls `medusa_decoder.generate()` which uses multiple Medusa heads to predict several tokens in parallel, rather than just one token at a time.

## 6. Modified `process_inference_result` in Node.py

**File:** `exo/orchestration/node.py`

**Change:** Added Medusa-specific output handling

```python
async def process_inference_result(
    self,
    shard,
    result: np.ndarray,
    request_id: Optional[str] = None,
    inference_state: Optional[dict] = None,
):
    # Check if this is a Medusa output and handle it specially
    if inference_state and inference_state.get("is_medusa_output", False):
        if DEBUG >= 1: print(f"[{request_id}] Processing Medusa output")
        
        # Get the medusa generated tokens
        medusa_output = inference_state.get("medusa_output")
        
        if medusa_output is not None:
            # Initialize buffered output if needed
            if request_id not in self.buffered_token_output:
                self.buffered_token_output[request_id] = ([], False)
                
            # Extract tokens as a flat list
            if len(medusa_output.shape) > 1:
                # Handle 2D array: first dim is batch, second is sequence
                tokens = medusa_output[0].tolist()
            else:
                # Handle 1D array
                tokens = medusa_output.tolist()
                
            # Set buffered output
            self.buffered_token_output[request_id] = (tokens, True)
            
            # Process tokens individually for proper streaming
            is_finished = True
            for i, token in enumerate(tokens):
                is_last = i == len(tokens) - 1
                self.trigger_on_token_callbacks(request_id, [token], is_last)
                # Small delay for smoother streaming
                if not is_last and i % 5 == 0:
                    await asyncio.sleep(0.01)
                    
            # Mark request as complete
            self.outstanding_requests.pop(request_id, None)
            
            # Return the full array of tokens
            return np.array(tokens)
    
    # Continue with existing non-Medusa handling...
```

**Purpose:** This modification processes the output from Medusa generation:
1. Detects if the result has the special `is_medusa_output` flag
2. Extracts the array of tokens from `medusa_output` 
3. Processes tokens individually to maintain streaming API
4. Returns the complete array as the result

**Example:** When Medusa generates multiple tokens at once (e.g., "What is the capital of France?" â†’ "The capital of France is Paris."), this method emits them one by one with small delays for smooth streaming to clients.

## 7. Added `load_checkpoint` Method

**File:** `exo/inference/torch/sharded_inference_engine.py`

**Change:** Implemented the required abstract method

```python
async def load_checkpoint(self, shard: Shard, path: str):
    """Implementation of the abstract method from InferenceEngine."""
    await self.ensure_shard(shard)
    
    def load_checkpoint_func():
        if DEBUG >= 1:
            print(f"Loading checkpoint from {path} for shard {shard}")
        
        try:
            # Placeholder implementation
            print(f"Warning: load_checkpoint not fully implemented for {self.__class__.__name__}")
            return True
        except Exception as e:
            print(f"Error loading checkpoint: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    return await asyncio.get_running_loop().run_in_executor(
        self.executor,
        load_checkpoint_func
    )
```

**Purpose:** This implements the abstract method required by the parent class. While not directly related to Medusa functionality, it's required for the class to be instantiable.

## How Medusa Actually Works

1. **Multiple Prediction Heads:** Medusa models have additional prediction heads (typically 4-8) that predict multiple future tokens in parallel.

2. **Tree Search:** The algorithm builds a tree of possible token sequences:
   - The base model generates the first token
   - Medusa heads predict the next several tokens in parallel
   - The algorithm builds candidate sequences from these predictions

3. **Verification:** The algorithm verifies these parallel predictions by feeding previous predictions back into the model to ensure quality.

4. **Acceptance:** Tokens that meet the quality threshold (posterior probability) are accepted without requiring individual model forwards.

## Example of Complete Flow

1. User executes: `exo run medusa-v1.0-vicuna-7b-v1.5 --inference-engine torch --medusa-enable --prompt "What is the capital of France?" --max-generate-tokens 50`

2. The framework detects: 
   - Model ID is "medusa-v1.0-vicuna-7b-v1.5"
   - `--medusa-enable` flag is present

3. In `ensure_shard`:
   - System checks model_cards registry and sees `"is_medusa": True`
   - `is_medusa_model_loaded` is set to `True`
   - Calls `_load_medusa_model`

4. In `_load_medusa_model`:
   - Model is loaded with `AutoModelForCausalLM` preserving Medusa heads
   - `MedusaDecoder` is created with the specified parameters
   - Model is wrapped in `MedusaModelWrapper` for API compatibility

5. In `infer_prompt`:
   - System detects Medusa is enabled
   - Calls specialized `_medusa_infer_prompt` method

6. In `_medusa_infer_prompt`:
   - Tokenizes the prompt
   - Calls `medusa_decoder.generate()` which predicts multiple tokens in parallel
   - Gets back array of tokens for complete response
   - Stores tokens in `inference_state["medusa_output"]`
   - Sets `inference_state["is_medusa_output"] = True`

7. In `process_inference_result`:
   - Detects `is_medusa_output = True`
   - Extracts tokens from `medusa_output`
   - Processes them sequentially for streaming
   - Returns full array as result

The result is significantly faster inference since multiple tokens are predicted in parallel rather than one-by-one as in standard inference.
